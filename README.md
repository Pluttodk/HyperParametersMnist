# HyperParametersMnist
Machine Learning has been a popular topic of research for the last several decades. The field of machine learning is notorious for the large amount of parameters which can be tuned, in order to provide better results. For this assignment we will be training artificial neural networks for image classification. We will focus on the impact of different batch sizes when training on a modern system with consumer-grade GPUs, and how different batch sizes affect the accuracy of various models. For our benchmarks, we will be using the MNIST dataset, which consists of 70.000, 28x28 gray scale images of hand-written digits, 0-9. http://yann.lecun.com/exdb/mnist/The dataset has a total uncompressed size of 54 MB. We will also be exploring different types of deep learning models with varying complexity, namely a fully connected network, a convolutional network (CNN), and a Residual Network (ResNet). Furthermore, we will be exploring the impacts on throughput and accuracy when training on one or on several high-end consumer-grade GPUs.
